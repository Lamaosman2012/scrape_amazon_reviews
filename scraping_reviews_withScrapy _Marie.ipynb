{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "1) from [This reference](https://www.datasciencecentral.com/how-to-scrape-amazon-product-data/)\n",
    "\n",
    "2) from [This reference](https://scrapfly.io/blog/how-to-scrape-amazon/)"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "source": [
    "import scrapy\n",
    "from scrapy import signals\n",
    "from scrapy.http import TextResponse \n",
    "import httpx\n",
    "from typing_extensions import TypedDict\n",
    "#from scrapy.xlib.pydispatch import dispatcher\n",
    "import asyncio"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "source": [
    "class AmazonSpider(scrapy.Spider):\n",
    "    name = 'amazon'\n",
    "    allowed_domains = ['amazon.com']\n",
    "    start_urls = ['http://www.amazon.com/']\n",
    "    def parse(self, response):\n",
    "        pass"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "source": [
    "query = [\"fridge\"]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "source": [
    "class AmazonSpider(scrapy.Spider):\n",
    "    def start_requests(self):\n",
    "        for query in queries:\n",
    "            url = 'https://www.amazon.com/s?' + urlencode({'k': query})\n",
    "            yield scrapy.Request(url=url, callback=self.parse_keyword_response)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "source": [
    "def parse_keyword_response(self, response):\n",
    "        products = response.xpath('//*[@data-asin]')\n",
    "        for product in products:\n",
    "            asin = product.xpath('@data-asin').extract_first()\n",
    "            product_url = f\"https://www.amazon.com/dp/{asin}\"\n",
    "            yield scrapy.Request(url=product_url, callback=self.parse_product_page, meta={'asin': asin})\n",
    "        next_page = response.xpath('//li[@class=\"a-last\"]/a/@href').extract_first()\n",
    "        if next_page:\n",
    "            url = urljoin(\"https://www.amazon.com\",next_page)\n",
    "            yield scrapy.Request(url=product_url, callback=self.parse_keyword_response)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "source": [
    "def parse_reviews(response): #-> ReviewData:\n",
    "    \"\"\"parse review from single review page\"\"\"\n",
    "    sel = Selector(text=response.text)\n",
    "    review_boxes = sel.css(\"#cm_cr-review_list div.review\")\n",
    "    parsed = []\n",
    "    for box in review_boxes:\n",
    "        parsed.append({\n",
    "                \"text\": \"\".join(box.css(\"span[data-hook=review-body] ::text\").getall()).strip(),\n",
    "                \"title\": box.css(\"*[data-hook=review-title]>span::text\").get(),\n",
    "                \"location_and_date\": box.css(\"span[data-hook=review-date] ::text\").get(),\n",
    "                \"verified\": bool(box.css(\"span[data-hook=avp-badge] ::text\").get()),\n",
    "                \"rating\": box.css(\"*[data-hook*=review-star-rating] ::text\").re(r\"(\\d+\\.*\\d*) out\")[0],\n",
    "        })\n",
    "    return parsed"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "source": [
    "class ReviewData(TypedDict):\n",
    "    \"\"\"storage type hint for amazons review object\"\"\"\n",
    "    title: str\n",
    "    text: str\n",
    "    location_and_date: str\n",
    "    verified: bool\n",
    "    rating: float"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "source": [
    "async def scrape_reviews(asin, session: httpx.AsyncClient) -> ReviewData:\n",
    "    \"\"\"scrape all reviews of a given ASIN of an amazon product\"\"\"\n",
    "    url = f\"https://www.amazon.com/product-reviews/{asin}/\"\n",
    "    log.info(f\"scraping review page: {url}\")\n",
    "    # find first page\n",
    "    first_page = await session.get(url)\n",
    "    sel = Selector(text=first_page.text)\n",
    "    # find total amount of pages \n",
    "    total_reviews = sel.css(\"div[data-hook=cr-filter-info-review-rating-count] ::text\").re(r\"(\\d+,*\\d*)\")[1]\n",
    "    total_reviews = int(total_reviews.replace(\",\", \"\"))\n",
    "    total_pages = int(math.ceil(total_reviews / 10.0))\n",
    "\n",
    "    log.info(f\"found total {total_reviews} reviews across {total_pages} pages -> scraping\")\n",
    "    _next_page = urljoin(url, sel.css(\".a-pagination .a-last>a::attr(href)\").get())\n",
    "    if _next_page:\n",
    "        next_page_urls = [_next_page.replace(\"pageNumber=2\", f\"pageNumber={i}\") for i in range(2, total_pages + 1)]\n",
    "        assert len(set(next_page_urls)) == len(next_page_urls)\n",
    "        other_pages = await asyncio.gather(*[session.get(url) for url in next_page_urls])\n",
    "    else:\n",
    "        other_pages = []\n",
    "    reviews = []\n",
    "    for response in [first_page, *other_pages]:\n",
    "        reviews.extend(parse_reviews(response))\n",
    "    log.info(f\"scraped total {len(reviews)} reviews\")\n",
    "    return reviews"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "source": [
    "import json\n",
    "import asyncio\n",
    "\n",
    "# We need to use browser-like headers for our requests to avoid being blocked\n",
    "# here we set headers of Chrome browser on Windows:\n",
    "BASE_HEADERS = {\n",
    "    \"accept-language\": \"en-US,en;q=0.9\",\n",
    "    \"user-agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/96.0.4664.110 Safari/537.36\",\n",
    "    \"accept\": \"text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,image/apng,*/*;q=0.8\",\n",
    "    \"accept-language\": \"en-US;en;q=0.9\",\n",
    "    \"accept-encoding\": \"gzip, deflate, br\",\n",
    "}"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "source": [
    "async def run():\n",
    "\n",
    "    limits = httpx.Limits(max_connections=5)\n",
    "    async with httpx.AsyncClient(limits=limits, timeout=httpx.Timeout(15.0), headers=BASE_HEADERS) as session:\n",
    "        data = await scrape_reviews(\"B08QVPBFCS\", session=session)\n",
    "   \n",
    "    print(json.dumps(data, indent=2))"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "source": [
    "async def main():\n",
    "    print(1)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "#https://stackoverflow.com/questions/55409641/asyncio-run-cannot-be-called-from-a-running-event-loop-when-using-jupyter-no\n",
    "#https://ipython.readthedocs.io/en/stable/interactive/autoawait.html#difference-between-terminal-ipython-and-ipykernel\n",
    "\n",
    "#asyncio.run(run())\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "_The last code block is throwing an error only in the ipython Notebook because it seams that it can not be run in the kernel_"
   ],
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.8 64-bit ('httpx': venv)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "c241aca8e82980483a0f0acfb405b3e1a2705bee13d54a1a3f8f2cfa39078674"
   }
  },
  "interpreter": {
   "hash": "33ed523ee00499345741ff5e0fa9ff5b8f6e48e56db554b40ff51fc912150421"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}