{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we will send a get request to the amazon API to retrieve the search results of a query therefore we will need to install the requests library and we need the url for the search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install requests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We replace the spaces in the search query (Keywords) by '+' "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "search_query = 'Refrigerator'.replace(' ', '+')\n",
    "# search_query = 't-shirt women'.replace(' ', '+') --> 't-shirt+women'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://www.amazon.com/s?k=Refrigerator&page=1\n"
     ]
    }
   ],
   "source": [
    "# The url of the search has always this standard format\n",
    "#  (with some optional extensions sometimes)\n",
    "search_url = f\"https://www.amazon.com/s?k={search_query}&page=1\"\n",
    "print(search_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "# The headers are useful to avoid getting blocked and to encode the content \n",
    "HEADERS = {\n",
    "    \"accept-language\": \"en-US,en;q=0.9\",\n",
    "    \"user-agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/96.0.4664.110 Safari/537.36\",\n",
    "    \"accept\": \"text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,image/apng,*/*;q=0.8\",\n",
    "    \"accept-encoding\": \"gzip, deflate, br\",\n",
    "}\n",
    "\n",
    "# the get request returns a response object which has a content and text methods\n",
    "\n",
    "response = requests.get(search_url, headers=HEADERS) # retrieve the results from the first page\n",
    "# check the type of the object\n",
    "print(type(response))\n",
    "# check the content and text methods\n",
    "# print(response.content)\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install httpx parsel loguru"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The parse_search  parses the items of any single page (of the response) of the search results but it skips the ads(sponsored results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[Documentation about the parsel library](https://parsel.readthedocs.io/en/v1.0.1/parsel.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The Selector module parses the response via css and httpx selectors which are usually used to style the html web page\n",
    "from parsel import Selector\n",
    "# The logger is used to show the colorful text in the run shell which gives information about the results and debugs the code\n",
    "from loguru import logger as log\n",
    "# The urljoin can be used to join urls after splitting them and to parse them\n",
    "from urllib.parse import urljoin \n",
    "\n",
    "# This function will parse the response page using the Selector\n",
    "# as an alternative of the beautiful soap\n",
    "# it takes any response page as an argument and returns  a list of dictionaries \n",
    "# of the titles and urls which we will use later to get the asin of the products and get the reviews\n",
    "def parse_search(resp):\n",
    "    \"\"\"Parse search result page for product previews\"\"\"\n",
    "    previews = []\n",
    "    sel = Selector(text=resp.text)\n",
    "\n",
    "    # find boxes of each product preview \n",
    "    \n",
    "    # Open the developer tool and inspect the results they will be \n",
    "    # inside div boxes with a class selector s-result-item)\n",
    "    product_boxes = sel.css(\"div.s-result-item[data-component-type=s-search-result]\")\n",
    "\n",
    "    for box in product_boxes:\n",
    "        # get the url of every search item in the search result\n",
    "        url = urljoin(str(resp.url), box.css(\"h2>a::attr(href)\").get()).split(\"?\")[0]\n",
    "\n",
    "        # print(urljoin(str(resp.url), box.css(\"h2>a::attr(href)\").get()).split(\"/\"))\n",
    "        # asin = urljoin(url, box.css(\"h2>a::attr(href)\").get()).split(\"/\")[5]\n",
    "        # print(asin)\n",
    "        if len(urljoin(str(resp.url), box.css(\"h2>a::attr(href)\").get()).split(\"/\"))!=6 and \"/slredirect/\" not in url and \"sspa\" not in url:  # skip ads etc.\n",
    "            # asin = urljoin(url, box.css(\"h2>a::attr(href)\").get()).split(\"/\")[5]\n",
    "            previews.append(\n",
    "                {\n",
    "                    \"url\": url,\n",
    "                    \"title\": box.css(\"h2>a>span::text\").get(),\n",
    "                    # \"asin\" : asin\n",
    "                }\n",
    "            )\n",
    "    log.debug(f\"found {len(previews)} product listings in {resp.url}\") # formulate the summery and debug log report\n",
    "    return previews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# main scope call the function to run it\n",
    "response = requests.get(search_url, headers=HEADERS)\n",
    "parse_search(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we need to get the results for the other pages.\n",
    "- We need to specify how many pages are they in total\n",
    "- we need to loop over those pages\n",
    "The reference had a bug in getting the total number of the results which have been fixed below\n",
    "The Function **search** is going to do this it takes a search query as argument and append the results to the list of .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# async def search(query, session):\n",
    "async def search(query):\n",
    "    \n",
    "    log.info(f\"{query}: scraping first page\")\n",
    "\n",
    "    # first, let's scrape first query page to find out how many pages we have in total:\n",
    "\n",
    "    # first_page = await session.get(f\"https://www.amazon.com/s?k={query}&page=1\")\n",
    "    search_url = f\"https://www.amazon.com/s?k={query}&page=1\"\n",
    "    first_page = requests.get(search_url, headers=HEADERS)\n",
    "    sel = Selector(text=first_page.text)\n",
    "    # print(sel.getall())\n",
    "    \"\"\"the following part of the tutorial was wrong and giving les pages than we should get\"\"\"\n",
    "    \"\"\"\n",
    "    _page_numbers = sel.xpath('//a[has-class(\"s-pagination-item\")][not(has-class(\"s-pagination-separator\"))]/text()').getall()# this is wronge from the reference\n",
    "    print(f\"page numbers{_page_numbers}\")\n",
    "    \"\"\"\n",
    "    last_page = sel.xpath('//span[has-class(\"s-pagination-disabled\")][not(has-class(\"s-pagination-previous\"))]/text()') # When you are on the first page the last page is without hyperlink i.e. no a selector and the previous page of the last do not appear in the span of the pagination list \n",
    "    # print(last_page.getall())\n",
    "    total_pages = int(last_page.getall()[0]) # the wrong solution was max(int(number) for number in _page_numbers)\n",
    "    # print(f\"total_pages are {total_pages}\")\n",
    "    log.info(f\"{query}: found {total_pages} pages, scraping them concurrently\")\n",
    "\n",
    "    # now we can scrape remaining pages concurrently \n",
    "    # (I commented out the async and the session to avoid the runtime error we will scrape them without awaiting time and without concurrency)\n",
    "    \"\"\"\n",
    "    other_pages = await asyncio.gather(\n",
    "         *[session.get(f\"https://www.amazon.com/s?k={query}&page={page}\") for page in range(2, total_pages + 1)]\n",
    "        )\n",
    "    \"\"\"\n",
    "    other_pages= []\n",
    "    for page_number in range(2, total_pages+1):\n",
    "        page = await asyncio.gather(requests.get(f\"https://www.amazon.com/s?k={query}&page={page_number}\", headers=HEADERS))\n",
    "        other_pages.extend(page)\n",
    "    # print(other_pages)\n",
    "    # print(len(other_pages))\n",
    "    # parse all search pages for product preview data:\n",
    "    previews = []\n",
    "    for response in [first_page, *other_pages]:\n",
    "        previews.extend(parse_search(response))\n",
    "\n",
    "    log.info(f\"{query}: found total of {len(previews)} product previews\")\n",
    "    return previews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#the asyncio library is to run the requests concurrently and not wait for the first to be done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install asyncio \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "query= 'Refrigerator'.replace(' ', '+')\n",
    "search(query)\n",
    "# asyncio.run(search(query))\n",
    "await search(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write the data to some json or Excel file \n",
    "def get_product_search_list(query):\n",
    "    data = search(query)\n",
    "        # for item in data:\n",
    "        #     print(item[\"asin\"])\n",
    "    i = int(input(\"Enter the file number four the output: \"))\n",
    "    with open(f'query_results_{i}.json', 'w') as file:\n",
    "        json.dump(data, file, indent=2)\n",
    "    # print(json.dumps(data, indent=2))  # this is an alternative to the above line to print the json dictionaries in the run shell\n",
    "\n",
    "    df = pd.DataFrame(data)\n",
    "    df.to_excel(f\"query_results_{i}.xlsx\", index=False)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# main scope run the function\n",
    "query= 'Refrigerator'.replace(' ', '+')\n",
    "get_product_search_list(query)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 ('venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "232ca6ad6c5cd8d24b74849b960539ffa85387a4b8074e21e4805db3ad34acf6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
