{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "from [This reference](https://scrapfly.io/blog/how-to-scrape-amazon/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup\n",
    "\n",
    "In this tutorial we'll be using Python and two major community packages:\n",
    "\n",
    "- httpx - HTTP client library which will let us communicate with amazon.com's servers\n",
    "- parsel - HTML parsing library which will help us to parse our web scraped HTML files. In this tutorial we'll be using a mixture of css and xpath selectors to parse HTML - both of which are supported by parsel.\n",
    "\n",
    "Optionally we'll also use: \n",
    "- loguru - a pretty logging library that'll help us keep track of what's going on.\n",
    "\n",
    "These packages can be easily installed via pip command:\n",
    "\n",
    "`pip install httpx parsel loguru`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scraping Reviews\n",
    "\n",
    "To scrape product reviews first let's take a look at where we can find them. If we scroll to the bottom of the page we can see a link that says \"See All Reviews\" and if we click it we can see that we are taken to a URL that follows this format:\n",
    "![review page](https://scrapfly.io/blog/content/images/how-to-scrape-amazon_url-reviews.svg)\n",
    "\n",
    "We can see that just like for product information all we need is the ASIN identifier to find the review page of a product. Let's add this logic to our scraper:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from typing_extensions import TypedDict\n",
    "import httpx\n",
    "from loguru import logger as log\n",
    "from parsel import Selector\n",
    "from urllib.parse import urljoin\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReviewData(TypedDict):\n",
    "    \"\"\"storage type hint for amazons review object\"\"\"\n",
    "    title: str\n",
    "    text: str\n",
    "    location_and_date: str\n",
    "    verified: bool\n",
    "    rating: float\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_reviews(response) -> ReviewData:\n",
    "    \"\"\"parse review from single review page\"\"\"\n",
    "    sel = Selector(text=response.text)\n",
    "    review_boxes = sel.css(\"#cm_cr-review_list div.review\")\n",
    "    parsed = []\n",
    "    for box in review_boxes:\n",
    "        parsed.append({\n",
    "                \"text\": \"\".join(box.css(\"span[data-hook=review-body] ::text\").getall()).strip(),\n",
    "                \"title\": box.css(\"*[data-hook=review-title]>span::text\").get(),\n",
    "                \"location_and_date\": box.css(\"span[data-hook=review-date] ::text\").get(),\n",
    "                \"verified\": bool(box.css(\"span[data-hook=avp-badge] ::text\").get()),\n",
    "                \"rating\": box.css(\"*[data-hook*=review-star-rating] ::text\").re(r\"(\\d+\\.*\\d*) out\")[0],\n",
    "        })\n",
    "    return parsed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def scrape_reviews(asin, session: httpx.AsyncClient) -> ReviewData:\n",
    "    \"\"\"scrape all reviews of a given ASIN of an amazon product\"\"\"\n",
    "    url = f\"https://www.amazon.com/product-reviews/{asin}/\"\n",
    "    log.info(f\"scraping review page: {url}\")\n",
    "    # find first page\n",
    "    first_page = await session.get(url)\n",
    "    sel = Selector(text=first_page.text)\n",
    "    # find total amount of pages \n",
    "    total_reviews = sel.css(\"div[data-hook=cr-filter-info-review-rating-count] ::text\").re(r\"(\\d+,*\\d*)\")[1]\n",
    "    total_reviews = int(total_reviews.replace(\",\", \"\"))\n",
    "    total_pages = int(math.ceil(total_reviews / 10.0))\n",
    "\n",
    "    log.info(f\"found total {total_reviews} reviews across {total_pages} pages -> scraping\")\n",
    "    _next_page = urljoin(url, sel.css(\".a-pagination .a-last>a::attr(href)\").get())\n",
    "    if _next_page:\n",
    "        next_page_urls = [_next_page.replace(\"pageNumber=2\", f\"pageNumber={i}\") for i in range(2, total_pages + 1)]\n",
    "        assert len(set(next_page_urls)) == len(next_page_urls)\n",
    "        other_pages = await asyncio.gather(*[session.get(url) for url in next_page_urls])\n",
    "    else:\n",
    "        other_pages = []\n",
    "    reviews = []\n",
    "    for response in [first_page, *other_pages]:\n",
    "        reviews.extend(parse_reviews(response))\n",
    "    log.info(f\"scraped total {len(reviews)} reviews\")\n",
    "    return reviews"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the above scraper we are putting together everything we've learned in this tutorial:\n",
    "\n",
    "To scrape pagination we are using the same technique we used in scraping search: scrape first page, find total pages and scrape the rest concurrently.\n",
    "To parse reviews are also using the same technique we used in parsing search: iterate through each box containing the review and parse the data using CSS selectors.\n",
    "\n",
    "Let's run this scraper and see what output it generates:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import asyncio\n",
    "\n",
    "# We need to use browser-like headers for our requests to avoid being blocked\n",
    "# here we set headers of Chrome browser on Windows:\n",
    "BASE_HEADERS = {\n",
    "    \"accept-language\": \"en-US,en;q=0.9\",\n",
    "    \"user-agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/96.0.4664.110 Safari/537.36\",\n",
    "    \"accept\": \"text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,image/apng,*/*;q=0.8\",\n",
    "    \"accept-language\": \"en-US;en;q=0.9\",\n",
    "    \"accept-encoding\": \"gzip, deflate, br\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def run():\n",
    "\n",
    "    limits = httpx.Limits(max_connections=5)\n",
    "    async with httpx.AsyncClient(limits=limits, timeout=httpx.Timeout(15.0), headers=BASE_HEADERS) as session:\n",
    "        data = await scrape_reviews(\"B08QVPBFCS\", session=session)\n",
    "   \n",
    "    print(json.dumps(data, indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "asyncio.run() cannot be called from a running event loop",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [11], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43masyncio\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/usr/lib/python3.10/asyncio/runners.py:33\u001b[0m, in \u001b[0;36mrun\u001b[0;34m(main, debug)\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[39m\"\"\"Execute the coroutine and return the result.\u001b[39;00m\n\u001b[1;32m     10\u001b[0m \n\u001b[1;32m     11\u001b[0m \u001b[39mThis function runs the passed coroutine, taking care of\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[39m    asyncio.run(main())\u001b[39;00m\n\u001b[1;32m     31\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m     32\u001b[0m \u001b[39mif\u001b[39;00m events\u001b[39m.\u001b[39m_get_running_loop() \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m---> 33\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mRuntimeError\u001b[39;00m(\n\u001b[1;32m     34\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39masyncio.run() cannot be called from a running event loop\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m     36\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m coroutines\u001b[39m.\u001b[39miscoroutine(main):\n\u001b[1;32m     37\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\u001b[39m\"\u001b[39m\u001b[39ma coroutine was expected, got \u001b[39m\u001b[39m{!r}\u001b[39;00m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(main))\n",
      "\u001b[0;31mRuntimeError\u001b[0m: asyncio.run() cannot be called from a running event loop"
     ]
    }
   ],
   "source": [
    "asyncio.run(run())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_The last code block is throwing an error only in the ipython Notebook because it seams that it can not be run in the kernel_"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 ('venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "c241aca8e82980483a0f0acfb405b3e1a2705bee13d54a1a3f8f2cfa39078674"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
