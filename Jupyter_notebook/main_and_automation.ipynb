{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Objective\n",
    "\n",
    "You need to start by reading the [scrape_search notebook](https://github.com/morschulik/scrape_amazon_reviews/blob/main/Jupyter_notebook/scrap_search.ipynb)\n",
    "\n",
    "Then the [scrape reviews](https://github.com/morschulik/scrape_amazon_reviews/blob/main/Jupyter_notebook/scrap_reviews.ipynb)\n",
    "\n",
    "\n",
    "In the main.py we want to combine both of them and create a function which retrieve all the products from a search query with its reviews and parse the results.\n",
    "\n",
    "\n",
    "This function will receive a search query(keyword) and return end result list combining the keyword, the title of the product the review details etc.\n",
    "\n",
    "In the automated scraper we have a trial to automatically run the code weekly, compares the recent results with the current ones and add write the results in a new file if there is a difference. \n",
    "Its aim is to keep a history of the data. The output could not be examined yet."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The main.py\n",
    "We need to import the functions from the scrap_search and scrape_reviews(make sure that those files are in the same directory like your notebook)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scraper.scrap_search import search\n",
    "from scraper.scrap_reviews import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the combining function will like following"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def get_reviews_by_keyword(keyword):\n",
    "    query = keyword.replace(' ', '+')\n",
    "    limits = httpx.Limits(max_connections=5)\n",
    "    async with httpx.AsyncClient(limits=limits, timeout=httpx.Timeout(20.0), headers=BASE_HEADERS) as session:\n",
    "\n",
    "        products_list = await search(query, session=session)\n",
    "        # asins_list = [product[\"asin\"] for product in products_list]\n",
    "        end_results_list = []\n",
    "        for product in products_list:\n",
    "            product_reviews = await scrape_reviews(product[\"asin\"], session=session)\n",
    "            for review in product_reviews:\n",
    "                end_results = {\n",
    "                    \"search keyword\": keyword,\n",
    "                    \"Product_asin\": product[\"asin\"],\n",
    "                    \"Product_title\": product[\"title\"],\n",
    "                    \"Product_url\": product[\"url\"],\n",
    "                    \"review_title\": review[\"title\"],\n",
    "                    \"review_rating\": review[\"star_rating\"],\n",
    "                    \"review_verified\": review[\"verified\"],\n",
    "                    \"review_location\": review[\"location\"],\n",
    "                    \"review_date\": review[\"date\"],\n",
    "                    \"review_body\": review[\"body\"],\n",
    "                }\n",
    "                end_results_list.append(end_results)\n",
    "    return end_results_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this code will take a while!\n",
    "search_keyword = 'Refrigerator'\n",
    "search_query = search_keyword.replace(' ', '+')\n",
    "reviews_by_keyword = await get_reviews_by_keyword(search_query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(reviews_by_keyword)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Write the data to JSON or pandas data frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = int(input(\"Enter the file number four the output: \"))\n",
    "\n",
    "# dump the data to json file\n",
    "with open(f'combined_results_{i}.json', 'w') as file:\n",
    "    json.dump(reviews_by_keyword, file, indent=2)\n",
    "# write the data to Excel file\n",
    "df = pd.DataFrame(reviews_by_keyword,\n",
    "                    columns=[\"search keyword\", \"Product_asin\", \"Product_title\", \"Product_url\", \"review_title\",\n",
    "                            \"review_rating\", \"review_verified\", \"review_location\", \"review_date\", \"review_body\", ])\n",
    "df.to_excel(f\"combined_results_{i}.xlsx\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The automated scraper\n",
    "Under examination. Still need to be checked better"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 ('venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "c241aca8e82980483a0f0acfb405b3e1a2705bee13d54a1a3f8f2cfa39078674"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
