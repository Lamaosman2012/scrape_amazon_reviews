{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main Reference \n",
    "[This Tutorial](https://scrapfly.io/blog/how-to-scrape-amazon/)\n",
    "\n",
    "But we debugged there code and added some other things"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Request vs response\n",
    "First we will usually send a get request to the amazon API to retrieve the search results of a search query therefore we will need to install the requests library and we need the url of the search site.\n",
    "\n",
    "`pip install requests`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We replace the spaces in the search query (Keywords) by '+' \n",
    "\n",
    "search_query = 'Refrigerator'.replace(' ', '+')\n",
    "# search_query = 't-shirt women'.replace(' ', '+') --> 't-shirt+women'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://www.amazon.com/s?k=Refrigerator&page=1\n"
     ]
    }
   ],
   "source": [
    "# The url of the search has always this standard format\n",
    "#  (with some optional extensions sometimes)\n",
    "search_url = f\"https://www.amazon.com/s?k={search_query}&page=1\"\n",
    "print(search_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "# We need to use browser-like headers for our requests to avoid being blocked and to encode the content of the response\n",
    "# here we set headers of Chrome browser on Windows\n",
    "\n",
    "HEADERS = {\n",
    "    \"accept-language\": \"en-US,en;q=0.9\",\n",
    "    \"user-agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/96.0.4664.110 Safari/537.36\",\n",
    "    \"accept\": \"text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,image/apng,*/*;q=0.8\",\n",
    "    \"accept-encoding\": \"gzip, deflate, br\",\n",
    "}\n",
    "\n",
    "# the get request returns a response object which has a content and text methods\n",
    "\n",
    "response = requests.get(search_url, headers=HEADERS) # retrieve the results from the first page\n",
    "# check the type of the object\n",
    "print(type(response))\n",
    "# check the content and text methods\n",
    "# print(response.content)\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup\n",
    "\n",
    "In this tutorial we'll be using Python and two major community packages:\n",
    "\n",
    "- httpx - HTTP client library which will let us communicate with amazon.com's servers\n",
    "- parsel - HTML parsing library which will help us to parse our web scraped HTML files. In this tutorial we'll be using a mixture of css and xpath selectors to parse HTML - both of which are supported by parsel.\n",
    "\n",
    "Optionally we'll also use: \n",
    "- loguru - a pretty logging library that'll help us keep track of what's going on.\n",
    "\n",
    "These packages can be easily installed via pip command:\n",
    "\n",
    "`pip install httpx parsel loguru`\n",
    "\n",
    "## More details:\n",
    "\n",
    "We need to parse the search results on the response object, we will use for this the parsel library which is [documented here](https://parsel.readthedocs.io/en/v1.0.1/parsel.html).\n",
    "\n",
    "We will use also a logger from the loguru library to provide some log information about the status of the execution of the scraper on the screen.\n",
    "\n",
    "Theses information will appear as colorful text in the run shell.\n",
    "\n",
    "The third library which we need is [the httpx library](https://www.python-httpx.org/).\n",
    "This is an http client and provides an alternative of the request library and  provides sync and async APIs, and support for both HTTP/1.1 and HTTP/2. \n",
    "\n",
    "When requesting the many results from a website the asyncronous approach is a better approach from the usual request and it retrieves the results [conurrently](https://stackoverflow.com/questions/5017392/what-does-concurrent-requests-really-mean) and awaite the fast processes this gives the http client the chance to get all the data.\n",
    "\n",
    "Install the above mentioned libraries if you did not install the whole requirements yet.\n",
    "\n",
    "The parse_search function below parses the items of any single page (of the response) of the search results but **it skips the ads(sponsored results)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The Selector module parses the response via css and httpx selectors which are usually used to style the html web page\n",
    "from parsel import Selector\n",
    "# The logger is used to show the colorful text in the run shell which gives information about the results and debugs the code\n",
    "from loguru import logger as log\n",
    "# The urljoin can be used to join urls after splitting them and to parse them\n",
    "from urllib.parse import urljoin \n",
    "\n",
    "# This function will parse the response page using the Selector\n",
    "# as an alternative of the beautiful soap\n",
    "# it takes any response page as an argument and returns  a list of dictionaries \n",
    "# of the titles and urls which we will use later to get the asin of the products and get the reviews\n",
    "def parse_search(resp):\n",
    "    \"\"\"Parse search result page for product previews\"\"\"\n",
    "    previews = []\n",
    "    sel = Selector(text=resp.text)\n",
    "\n",
    "    # find boxes of each product preview \n",
    "    \n",
    "    # Open the developer tool and inspect the results they will be \n",
    "    # inside div boxes with a class selector s-result-item)\n",
    "    product_boxes = sel.css(\"div.s-result-item[data-component-type=s-search-result]\")\n",
    "\n",
    "    for box in product_boxes:\n",
    "        asin = box.xpath('@data-asin').extract_first()\n",
    "        # get the url of every search item in the search result, \n",
    "        # these include also sponsored items and ads, these will have the string \"/slredirect/\" \n",
    "        box_url = urljoin(str(resp.url), box.css(\"h2>a::attr(href)\").get()).split(\"?\")[0]\n",
    "        \n",
    "        # the standard url of any product is:\n",
    "        url = f\"https://www.amazon.com/dp/{asin}\" \n",
    "   \n",
    "        if len(urljoin(str(resp.url), box.css(\"h2>a::attr(href)\").get()).split(\"/\"))!=6 and \"/slredirect/\" not in url and \"sspa\" not in url:  # skip ads etc.\n",
    "            previews.append(\n",
    "                {\n",
    "                  \"asin\": asin,\n",
    "                    \"title\": box.css(\"h2>a>span::text\").get(),\n",
    "                    \"url\": url,\n",
    "                }\n",
    "            )\n",
    "    log.debug(f\"found {len(previews)} product listings in {resp.url}\") # formulate the summery and debug log report\n",
    "    return previews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# main scope call the function to run it\n",
    "response = requests.get(search_url, headers=HEADERS) # search_url is declared in the first part\n",
    "parse_search(response)\n",
    "# this parse_search works for any kind of response be it from a get request or from httpx client"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Httpx Client"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us try the search_parse with response from an httpx client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import httpx\n",
    "response= httpx.get(search_url, headers=HEADERS) # you will get an error about the url \n",
    "parse_search(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Httpx AsyncClient\n",
    "\n",
    "From [Async Support](https://www.python-httpx.org/async/)\n",
    "\n",
    "HTTPX offers a standard synchronous API by default, but also gives you the option of an async client if you need it.\n",
    "\n",
    "Async is a concurrency model that is far more efficient than multi-threading, and can provide significant performance benefits and enable the use of long-lived network connections such as WebSockets.\n",
    "\n",
    "If you're working with an async web framework then you'll also want to use an async client for sending outgoing HTTP requests.\n",
    "# Making Async requests\n",
    "\n",
    "To make asynchronous requests, you'll need an AsyncClient.\n",
    "\n",
    "See the above reference to understand the async requests better.\n",
    "\n",
    "Here is an example of the syntax."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<Response [200 OK]>\n"
     ]
    }
   ],
   "source": [
    "async with httpx.AsyncClient() as client:\n",
    "   r = await client.get('https://www.example.com/')\n",
    "print(r)   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following we  will call the async Client as 'session'\n",
    "\n",
    "Let see to parse the response if this session using our parse_search function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "session= httpx.AsyncClient()\n",
    "first_page = await session.get(search_url, headers=HEADERS)\n",
    "parse_search(first_page)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get the results for the other pages\n",
    "So until now we tried the parse search with search_url of the first page and it worked for asyncClient session \n",
    "\n",
    "Now we need to get the results for the other pages.\n",
    "- We need to specify how many pages are they in total\n",
    "- we need to loop over those pages\n",
    "The reference had a bug in getting the total number of the results which have been fixed below\n",
    "The Function **search** is going to do this it takes a search query as argument and append the results to the list."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the asyncio library is to run the requests concurrently and to wait the fast requests until the slower ones get run.\n",
    "\n",
    "`pip install asyncio`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "async def search(query:str, session:httpx.AsyncClient):\n",
    "    \n",
    "    log.info(f\"{query}: scraping first page\")\n",
    "\n",
    "    # first, let's scrape first query page to find out how many pages we have in total:\n",
    "    search_url = f\"https://www.amazon.com/s?k={query}&page=1\"\n",
    "    first_page = await session.get(search_url, headers=HEADERS)\n",
    "    sel = Selector(text=first_page.text)\n",
    "    # print(sel.getall())\n",
    "\n",
    "    \"\"\"the following part of the tutorial was wrong and giving les pages than we should get\"\"\"\n",
    "    \"\"\"\n",
    "    _page_numbers = sel.xpath('//a[has-class(\"s-pagination-item\")][not(has-class(\"s-pagination-separator\"))]/text()').getall()# this is wronge from the reference\n",
    "    print(f\"page numbers{_page_numbers}\")\n",
    "    \"\"\"\n",
    "    last_page = sel.xpath('//span[has-class(\"s-pagination-disabled\")][not(has-class(\"s-pagination-previous\"))]/text()') # When you are on the first page the last page is without hyperlink i.e. no a selector and the previous page of the last do not appear in the span of the pagination list \n",
    "    # print(last_page.getall())\n",
    "    total_pages = int(last_page.getall()[0]) # the wrong solution was max(int(number) for number in _page_numbers)\n",
    "    # print(f\"total_pages are {total_pages}\")\n",
    "    log.info(f\"{query}: found {total_pages} pages, scraping them concurrently\")\n",
    "\n",
    "    # now we can scrape remaining pages concurrently \n",
    "    # (I commented out the async and the session to avoid the runtime error we will scrape them without awaiting time and without concurrency)\n",
    "    \"\"\"\n",
    "    other_pages = await asyncio.gather(\n",
    "         *[session.get(f\"https://www.amazon.com/s?k={query}&page={page}\") for page in range(2, total_pages + 1)]\n",
    "        )\n",
    "    \"\"\"\n",
    "    other_pages= []\n",
    "    for page_number in range(2, total_pages+1):\n",
    "        page = await asyncio.gather(session.get(f\"https://www.amazon.com/s?k={query}&page={page_number}\", headers=HEADERS))\n",
    "        other_pages.extend(page)\n",
    "    # print(other_pages)\n",
    "    # print(len(other_pages))\n",
    "    # parse all search pages for product preview data:\n",
    "    previews = []\n",
    "    for response in [first_page, *other_pages]:\n",
    "        previews.extend(parse_search(response))\n",
    "\n",
    "    log.info(f\"{query}: found total of {len(previews)} product previews\")\n",
    "    return previews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "query= 'Refrigerator'.replace(' ', '+')\n",
    "# search(query)\n",
    "# asyncio.run(search(query))\n",
    "parsed= await search(query, session)\n",
    "# if you want to print the results do it in another sell to avoid connect time out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(parsed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above code \n",
    "\n",
    "Can be run in jupyter Notebook cells or in the python console but not inside a python script. \n",
    "\n",
    "Python will give you an error message which says that you can use the await outside an async function.\n",
    "\n",
    "Therefore the next step is important to run the search function without errors in python. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def get_product_search_list(query):\n",
    "    limits = httpx.Limits(max_connections=5)\n",
    "    async with httpx.AsyncClient(limits=limits, timeout=httpx.Timeout(15.5), headers=HEADERS) as session:\n",
    "        parsed_data = await search(query, session=session)\n",
    "    return parsed_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parsed2 = await get_product_search_list(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(parsed2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Write the data into a jSON file and a pandas DataFrame (Excel file)\n",
    "\n",
    "A pop up will ask you to provide i (for file number to avoid overwriting other files when running the code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write the data to some json or Excel file \n",
    "i = int(input(\"Enter the file number four the output: \"))\n",
    "\n",
    "# dump to a json file\n",
    "with open(f'query_results_{i}.json', 'w') as file:\n",
    "    json.dump(parsed, file, indent=2)\n",
    "# print(json.dumps(data, indent=2))  # this is an alternative to the above line to print the json dictionaries in the run shell\n",
    "\n",
    "# write to excel\n",
    "df = pd.DataFrame(parsed)\n",
    "df.to_excel(f\"query_results_{i}.xlsx\", index=False)\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 ('venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "c241aca8e82980483a0f0acfb405b3e1a2705bee13d54a1a3f8f2cfa39078674"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
