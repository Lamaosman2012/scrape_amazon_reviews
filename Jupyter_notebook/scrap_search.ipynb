{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we will usually send a get request to the amazon API to retrieve the search results of a search query therefore we will need to install the requests library and we need the url of the search site.\n",
    "\n",
    "`pip install requests`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install requests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We replace the spaces in the search query (Keywords) by '+' \n",
    "\n",
    "search_query = 'Refrigerator'.replace(' ', '+')\n",
    "# search_query = 't-shirt women'.replace(' ', '+') --> 't-shirt+women'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://www.amazon.com/s?k=Refrigerator&page=1\n"
     ]
    }
   ],
   "source": [
    "# The url of the search has always this standard format\n",
    "#  (with some optional extensions sometimes)\n",
    "search_url = f\"https://www.amazon.com/s?k={search_query}&page=1\"\n",
    "print(search_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "# We need to use browser-like headers for our requests to avoid being blocked and to encode the content of the response\n",
    "# here we set headers of Chrome browser on Windows\n",
    "\n",
    "HEADERS = {\n",
    "    \"accept-language\": \"en-US,en;q=0.9\",\n",
    "    \"user-agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/96.0.4664.110 Safari/537.36\",\n",
    "    \"accept\": \"text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,image/apng,*/*;q=0.8\",\n",
    "    \"accept-encoding\": \"gzip, deflate, br\",\n",
    "}\n",
    "\n",
    "# the get request returns a response object which has a content and text methods\n",
    "\n",
    "response = requests.get(search_url, headers=HEADERS) # retrieve the results from the first page\n",
    "# check the type of the object\n",
    "print(type(response))\n",
    "# check the content and text methods\n",
    "# print(response.content)\n",
    "print(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we need to parse the search results on the response object, we will use for this the parsel library which is [documented here](https://parsel.readthedocs.io/en/v1.0.1/parsel.html).\n",
    "\n",
    "We will use also a logger from the loguru library to provide some log information about the status of the execution of the scraper on the screen.\n",
    "\n",
    "Theses information will appear as colorful text in the run shell.\n",
    "\n",
    "The third library which we need is [the httpx library](https://www.python-httpx.org/).\n",
    "This is an http client and provides an alternative of the request library and  provides sync and async APIs, and support for both HTTP/1.1 and HTTP/2. \n",
    "\n",
    "When requesting the many results from a website the asyncronous approach is a better approach from the usual request and it retrieves the results [conurrently](https://stackoverflow.com/questions/5017392/what-does-concurrent-requests-really-mean) and awaite the fast processes this gives the http client the chance to get all the data.\n",
    "\n",
    "Install the above mentioned libraries if you did not install the whole requirements yet.\n",
    "\n",
    "`pip install httpx parsel loguru`\n",
    "\n",
    "The parse_search function below parses the items of any single page (of the response) of the search results but **it skips the ads(sponsored results)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The Selector module parses the response via css and httpx selectors which are usually used to style the html web page\n",
    "from parsel import Selector\n",
    "# The logger is used to show the colorful text in the run shell which gives information about the results and debugs the code\n",
    "from loguru import logger as log\n",
    "# The urljoin can be used to join urls after splitting them and to parse them\n",
    "from urllib.parse import urljoin \n",
    "\n",
    "# This function will parse the response page using the Selector\n",
    "# as an alternative of the beautiful soap\n",
    "# it takes any response page as an argument and returns  a list of dictionaries \n",
    "# of the titles and urls which we will use later to get the asin of the products and get the reviews\n",
    "def parse_search(resp):\n",
    "    \"\"\"Parse search result page for product previews\"\"\"\n",
    "    previews = []\n",
    "    sel = Selector(text=resp.text)\n",
    "\n",
    "    # find boxes of each product preview \n",
    "    \n",
    "    # Open the developer tool and inspect the results they will be \n",
    "    # inside div boxes with a class selector s-result-item)\n",
    "    product_boxes = sel.css(\"div.s-result-item[data-component-type=s-search-result]\")\n",
    "\n",
    "    for box in product_boxes:\n",
    "        # get the url of every search item in the search result\n",
    "        url = urljoin(str(resp.url), box.css(\"h2>a::attr(href)\").get()).split(\"?\")[0]\n",
    "\n",
    "        # print(urljoin(str(resp.url), box.css(\"h2>a::attr(href)\").get()).split(\"/\"))\n",
    "        # asin = urljoin(url, box.css(\"h2>a::attr(href)\").get()).split(\"/\")[5]\n",
    "        # print(asin)\n",
    "        if len(urljoin(str(resp.url), box.css(\"h2>a::attr(href)\").get()).split(\"/\"))!=6 and \"/slredirect/\" not in url and \"sspa\" not in url:  # skip ads etc.\n",
    "            # asin = urljoin(url, box.css(\"h2>a::attr(href)\").get()).split(\"/\")[5]\n",
    "            previews.append(\n",
    "                {\n",
    "                    \"url\": url,\n",
    "                    \"title\": box.css(\"h2>a>span::text\").get(),\n",
    "                    # \"asin\" : asin\n",
    "                }\n",
    "            )\n",
    "    log.debug(f\"found {len(previews)} product listings in {resp.url}\") # formulate the summery and debug log report\n",
    "    return previews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-10-14 01:16:12.811 | DEBUG    | __main__:parse_search:39 - found 16 product listings in https://www.amazon.com/s?k=Refrigerator&page=1\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'url': 'https://www.amazon.com/Galanz-Retro-Refrigerator-Mounted-STAR/dp/B07QYXCFLW/ref=sr_1_2',\n",
       "  'title': 'Galanz GLR10TBKEFR True Top Freezer Retro Refrigerator Frost Free, Dual Door Fridge, Adjustable Electrical Thermostat Control, Black, 10.0 Cu Ft'},\n",
       " {'url': 'https://www.amazon.com/Frigidaire-CUREFR331BK-Cubic-ft-Eraser-Fridge/dp/B07NJ8SM6D/ref=sr_1_3',\n",
       "  'title': 'FRIGIDAIRE EFR331-BLACK 3.2 Cu ft Eraser Board Mini Compact Dorm Fridge (Black)'},\n",
       " {'url': 'https://www.amazon.com/Frigidaire-EFR376-BLACK-Fridge-Bottle-Opener/dp/B07LDWLNVF/ref=sr_1_4',\n",
       "  'title': 'FRIGIDAIRE EFR376-BLACK 3.1 Cu Ft Black Retro Bar Fridge with Side Bottle Opener'},\n",
       " {'url': 'https://www.amazon.com/WHD-113FSS1-Freezer-Adjustable-Refrigerator-Stainless/dp/B00MWXSFM8/ref=sr_1_5',\n",
       "  'title': 'Midea WHD-113FSS1 Compact Refrigerator, 3.1 cu ft, Stainless Steel'},\n",
       " {'url': 'https://www.amazon.com/Frigidaire-EFMIS129-Portable-Personal-Freon-Free/dp/B07KZLJ7PB/ref=sr_1_6',\n",
       "  'title': 'Frigidaire RED EFMIS129- CP4 Mini Portable Compact Personal Fridge Cooler, 4 Liter Capacity Chills Six 12 oz Cans, 100% Freon-Free & Eco Friendly, Includes Plugs for Home Outlet & 12V Car Charger'},\n",
       " {'url': 'https://www.amazon.com/Anukis-Compact-Refrigerator-Apartment-Basement/dp/B09N1CWGG5/ref=sr_1_7',\n",
       "  'title': 'Anukis Compact Refrigerator 3.5 Cu Ft 2 Door Mini Fridge with Freezer For Apartment, Dorm, Office, Family, Basement, Garage, Silver'},\n",
       " {'url': 'https://www.amazon.com/Portable-Thermoelectric-Refrigerators-Skincare-Removable/dp/B0BCFDZWPN/ref=sr_1_8',\n",
       "  'title': '15 Liter Mini Fridge for Bedroom, 110V AC/ 12V DC Portable Thermoelectric Cooler and Warmer Small Refrigerators for Home, Dorm, Office and Car, Skincare, Cosmetic, Beverage, Removable shelf, Low Noise'},\n",
       " {'url': 'https://www.amazon.com/Portable-Skincare-Cosmetics-Refrigerator-Mirrored/dp/B08DHJQD6G/ref=sr_1_9',\n",
       "  'title': 'Mini Fridge 6 Liter Portable Beauty Makeup Skincare Fridge Cosmetics Refrigerator Compact Cooler Warmer for Bedroom, Office, Car, Dorm, Mirrored and Led Lighting Design AC/DC'},\n",
       " {'url': 'https://www.amazon.com/WHS-65LB1-Compact-Single-Reversible-Refrigerator/dp/B00MVVI1FC/ref=sr_1_10',\n",
       "  'title': 'Midea WHS-65LB1 Compact Single Reversible Door Refrigerator, 1.6 Cubic Feet(0.045 Cubic Meter), Black'},\n",
       " {'url': 'https://www.amazon.com/ENVENTOR-Skincare-Portable-Thermoelectric-Refrigerators/dp/B09SV5HQ29/ref=sr_1_11',\n",
       "  'title': 'Mini Fridge for Bedroom, ENVENTOR Skincare Fridge, 4 Liter/6 Can Mini Portable Thermoelectric Cooler and Warmer Refrigerators for Skincare, Foods, Office, Travel and Car(White)'},\n",
       " {'url': 'https://www.amazon.com/Frigidaire-EFR451-Refrigerator-Platinum-Stainless/dp/B088G39HRF/ref=sr_1_13',\n",
       "  'title': 'Frigidaire EFR451 2 Door Refrigerator/Freezer, 4.6 cu ft, Platinum Series, Stainless Steel, Double'},\n",
       " {'url': 'https://www.amazon.com/COOSEON-Portable-Skincare-Thermoelectric-Refrigerators/dp/B09LQF14WF/ref=sr_1_14',\n",
       "  'title': '7 Liter Mini Fridge LED Adjustable Mirrored Light Beauty Fridge & 3 Storage Shelf, COOSEON AC/DC Thermoelectric Cooler & Warmer Skincare Makeup Fridge for Bedroom, Room, Office, Car'},\n",
       " {'url': 'https://www.amazon.com/WRE110-Skincare-Refrigerator-Cosmetic-Beverage/dp/B0BBZFPQRL/ref=sr_1_15',\n",
       "  'title': 'ECOWELL WRE110 Mini, 4 Liter/6 Can Skincare Fridge with LED Mirror, AC/DC Cooler & Warmer Small Refrigerator for Skin Care Cosmetic Makeup for Bedroom Dorm Car Office Desk, White'},\n",
       " {'url': 'https://www.amazon.com/Whynter-BR-130SB-Beverage-Refrigerator-Stainless/dp/B00P7QI4IM/ref=sr_1_16',\n",
       "  'title': 'Whynter BR-130SB Internal Fan Beverage Refrigerators, Black/Stainless Steel'},\n",
       " {'url': 'https://www.amazon.com/Frigidaire-Compact-Beverage-Refrigerator-keeping/dp/B07K6YVF49/ref=sr_1_18',\n",
       "  'title': 'Frigidaire Retro Mini Fridge, 12/ 9 liters Cans Beverage Cooler, 100% Freon-Free & Eco Friendly Perfect for Home, Office, or Cars. Includes Plugs for Home Outlet & 12V Car Charger - Blue'},\n",
       " {'url': 'https://www.amazon.com/BLACK-DECKER-BCRK32B-Compact-Refrigerator/dp/B01DZQI6W4/ref=sr_1_19',\n",
       "  'title': 'BLACK+DECKER BCRK32B Compact Refrigerator Energy Star Single Door Mini Fridge with Freezer, 3.2 Cubic Feet, Black'}]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# main scope call the function to run it\n",
    "response = requests.get(search_url, headers=HEADERS)\n",
    "parse_search(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we need to get the results for the other pages.\n",
    "- We need to specify how many pages are they in total\n",
    "- we need to loop over those pages\n",
    "The reference had a bug in getting the total number of the results which have been fixed below\n",
    "The Function **search** is going to do this it takes a search query as argument and append the results to the list of .."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# async def search(query, session):\n",
    "async def search(query):\n",
    "    \n",
    "    log.info(f\"{query}: scraping first page\")\n",
    "\n",
    "    # first, let's scrape first query page to find out how many pages we have in total:\n",
    "\n",
    "    # first_page = await session.get(f\"https://www.amazon.com/s?k={query}&page=1\")\n",
    "    search_url = f\"https://www.amazon.com/s?k={query}&page=1\"\n",
    "    first_page = requests.get(search_url, headers=HEADERS)\n",
    "    sel = Selector(text=first_page.text)\n",
    "    # print(sel.getall())\n",
    "    \"\"\"the following part of the tutorial was wrong and giving les pages than we should get\"\"\"\n",
    "    \"\"\"\n",
    "    _page_numbers = sel.xpath('//a[has-class(\"s-pagination-item\")][not(has-class(\"s-pagination-separator\"))]/text()').getall()# this is wronge from the reference\n",
    "    print(f\"page numbers{_page_numbers}\")\n",
    "    \"\"\"\n",
    "    last_page = sel.xpath('//span[has-class(\"s-pagination-disabled\")][not(has-class(\"s-pagination-previous\"))]/text()') # When you are on the first page the last page is without hyperlink i.e. no a selector and the previous page of the last do not appear in the span of the pagination list \n",
    "    # print(last_page.getall())\n",
    "    total_pages = int(last_page.getall()[0]) # the wrong solution was max(int(number) for number in _page_numbers)\n",
    "    # print(f\"total_pages are {total_pages}\")\n",
    "    log.info(f\"{query}: found {total_pages} pages, scraping them concurrently\")\n",
    "\n",
    "    # now we can scrape remaining pages concurrently \n",
    "    # (I commented out the async and the session to avoid the runtime error we will scrape them without awaiting time and without concurrency)\n",
    "    \"\"\"\n",
    "    other_pages = await asyncio.gather(\n",
    "         *[session.get(f\"https://www.amazon.com/s?k={query}&page={page}\") for page in range(2, total_pages + 1)]\n",
    "        )\n",
    "    \"\"\"\n",
    "    other_pages= []\n",
    "    for page_number in range(2, total_pages+1):\n",
    "        page = await asyncio.gather(requests.get(f\"https://www.amazon.com/s?k={query}&page={page_number}\", headers=HEADERS))\n",
    "        other_pages.extend(page)\n",
    "    # print(other_pages)\n",
    "    # print(len(other_pages))\n",
    "    # parse all search pages for product preview data:\n",
    "    previews = []\n",
    "    for response in [first_page, *other_pages]:\n",
    "        previews.extend(parse_search(response))\n",
    "\n",
    "    log.info(f\"{query}: found total of {len(previews)} product previews\")\n",
    "    return previews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#the asyncio library is to run the requests concurrently and not wait for the first to be done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install asyncio \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "query= 'Refrigerator'.replace(' ', '+')\n",
    "search(query)\n",
    "# asyncio.run(search(query))\n",
    "await search(query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write the data to some json or Excel file \n",
    "def get_product_search_list(query):\n",
    "    data = search(query)\n",
    "        # for item in data:\n",
    "        #     print(item[\"asin\"])\n",
    "    i = int(input(\"Enter the file number four the output: \"))\n",
    "    with open(f'query_results_{i}.json', 'w') as file:\n",
    "        json.dump(data, file, indent=2)\n",
    "    # print(json.dumps(data, indent=2))  # this is an alternative to the above line to print the json dictionaries in the run shell\n",
    "\n",
    "    df = pd.DataFrame(data)\n",
    "    df.to_excel(f\"query_results_{i}.xlsx\", index=False)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# main scope run the function\n",
    "query= 'Refrigerator'.replace(' ', '+')\n",
    "get_product_search_list(query)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 ('venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "c241aca8e82980483a0f0acfb405b3e1a2705bee13d54a1a3f8f2cfa39078674"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
