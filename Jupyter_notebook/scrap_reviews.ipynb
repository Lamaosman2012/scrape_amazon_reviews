{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main Reference\n",
    "from [This reference](https://scrapfly.io/blog/how-to-scrape-amazon/)\n",
    "But we debugged their code and added other things"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup\n",
    "\n",
    "In this tutorial we'll be using Python and two major community packages:\n",
    "\n",
    "- httpx - HTTP client library which will let us communicate with amazon.com's servers\n",
    "- parsel - HTML parsing library which will help us to parse our web scraped HTML files. In this tutorial we'll be using a mixture of css and xpath selectors to parse HTML - both of which are supported by parsel.\n",
    "\n",
    "Optionally we'll also use: \n",
    "- loguru - a pretty logging library that'll help us keep track of what's going on.\n",
    "\n",
    "These packages can be easily installed via pip command:\n",
    "\n",
    "`pip install httpx parsel loguru`\n",
    "\n",
    "See [our scrape search notebook](https://github.com/morschulik/scrape_amazon_reviews/blob/main/Jupyter_notebook/scrap_search.ipynb) \n",
    "\n",
    "We advice you to start from there to understand the logic used below better a more details on those libraries."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scraping Reviews\n",
    "\n",
    "To scrape product reviews first let's take a look at where we can find them. If we scroll to the bottom of the page we can see a link that says \"See All Reviews\" and if we click it we can see that we are taken to a URL that follows this format:\n",
    "![review page](https://scrapfly.io/blog/content/images/how-to-scrape-amazon_url-reviews.svg)\n",
    "\n",
    "or more shortly:\n",
    "`url = f\"https://www.amazon.com/product-reviews/{asin}/\"`\n",
    "\n",
    "We can see that just like for product information all we need is the ASIN identifier to find the review page of a product. Let's add this logic to our scraper:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from typing_extensions import TypedDict\n",
    "import httpx\n",
    "from loguru import logger as log\n",
    "from parsel import Selector\n",
    "from urllib.parse import urljoin\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "you can send to the following function any response(to the get reviews request) and it will parse it to a list of dictionaries having title , body, location, date etc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_reviews(response):\n",
    "    \"\"\"parse review from single review page\"\"\"\n",
    "    sel = Selector(text=response.text)\n",
    "    review_boxes = sel.css(\"#cm_cr-review_list div.review\") # inspect them in the browser\n",
    "    parsed = []\n",
    "    for box in review_boxes:\n",
    "        parsed.append({\n",
    "                \"body\": \"\".join(box.css(\"span[data-hook=review-body] ::text\").getall()).strip(),\n",
    "                \"title\": box.css(\"*[data-hook=review-title]>span::text\").get(),\n",
    "                \"location\": box.css(\"span[data-hook=review-date] ::text\").get().split(\" on\")[0].replace(\"Reviewed in \", \"\"), # split location from date and delete the reviewd in string\n",
    "                \"date\": box.css(\"span[data-hook=review-date] ::text\").get().split(\" on\")[1],\n",
    "                \"verified\": bool(box.css(\"span[data-hook=avp-badge] ::text\").get()),\n",
    "                \"star_rating\": box.css(\"*[data-hook*=review-star-rating] ::text\").re(r\"(\\d+\\.*\\d*) out\")[0],\n",
    "        })\n",
    "    return parsed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us run it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "asin='B08QVPBFCS'\n",
    "response= httpx.get(url = f\"https://www.amazon.com/product-reviews/{asin}/\") # the response returns the first page\n",
    "parsed = parse_reviews(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(parsed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following function:\n",
    "- takes the asin (like a product id which is in the url) as an argument.\n",
    "- an httpx.AsyncClient session as another argument (to retrieve the reviews concurrently).\n",
    "- uses the the response which results from the httpx request as an argument of the parse_review function to to parse the first page.\n",
    "- goes throw all the pages\n",
    "- finally saves the data in a list and return it.\n",
    "\n",
    "To parse reviews we are also using the same technique we used in parsing search: iterate through each box containing the review and parse the data using CSS selectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "async def scrape_reviews(asin, session: httpx.AsyncClient):# -> ReviewData:\n",
    "    \"\"\"scrape all reviews of a given ASIN of an amazon product\"\"\"\n",
    "    url = f\"https://www.amazon.com/product-reviews/{asin}/\"\n",
    "    log.info(f\"scraping review page: {url}\")\n",
    "    # find first page\n",
    "    first_page = await session.get(url)\n",
    "    sel = Selector(text=first_page.text)\n",
    "    # find total amount of pages \n",
    "    total_reviews = sel.css(\"div[data-hook=cr-filter-info-review-rating-count] ::text\").re(r\"(\\d+,*\\d*)\")[1]\n",
    "    total_reviews = int(total_reviews.replace(\",\", \"\"))\n",
    "    total_pages = int(math.ceil(total_reviews / 10.0))\n",
    "\n",
    "    log.info(f\"found total {total_reviews} reviews across {total_pages} pages -> scraping\")\n",
    "    _next_page = urljoin(url, sel.css(\".a-pagination .a-last>a::attr(href)\").get())\n",
    "    if _next_page:\n",
    "        next_page_urls = [_next_page.replace(\"pageNumber=2\", f\"pageNumber={i}\") for i in range(2, total_pages + 1)]\n",
    "        assert len(set(next_page_urls)) == len(next_page_urls)\n",
    "        other_pages = await asyncio.gather(*[session.get(url) for url in next_page_urls])\n",
    "    else:\n",
    "        other_pages = []\n",
    "    reviews = []\n",
    "    for response in [first_page, *other_pages]:\n",
    "        reviews.extend(parse_reviews(response))\n",
    "    log.info(f\"scraped total {len(reviews)} reviews\")\n",
    "    return reviews"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's run this scraper and see what output it generates:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "product_asin = 'B08QVPBFCS'\n",
    "session=session= httpx.AsyncClient()\n",
    "parsed_reviews= await scrape_reviews(product_asin, session)\n",
    "# if you want to print the results do it in another sell to avoid connect time out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(parsed_reviews)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lt us set limits and connection timeout to the session and run the function in a way that works in python scripts(in python scripts you are not allowed to use the await outside a async function )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import asyncio\n",
    "\n",
    "# We need to use browser-like headers for our requests to avoid being blocked\n",
    "# here we set headers of Chrome browser on Windows:\n",
    "BASE_HEADERS = {\n",
    "    \"accept-language\": \"en-US,en;q=0.9\",\n",
    "    \"user-agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/96.0.4664.110 Safari/537.36\",\n",
    "    \"accept\": \"text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,image/apng,*/*;q=0.8\",\n",
    "    \"accept-language\": \"en-US;en;q=0.9\",\n",
    "    \"accept-encoding\": \"gzip, deflate, br\",\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "async def get_reviews(some_asin):\n",
    "\n",
    "    limits = httpx.Limits(max_connections=5)\n",
    "    async with httpx.AsyncClient(limits=limits, timeout=httpx.Timeout(15.0), headers=BASE_HEADERS) as session:\n",
    "        review_data = await scrape_reviews(\"B08QVPBFCS\", session=session)\n",
    "    return review_data\n",
    "    # print(json.dumps(data, indent=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# to run it in a python script use asyncio.run(run()) \n",
    "some_asin =\"B08QVPBFCS\"\n",
    "review_data = await get_reviews(some_asin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(review_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_The last code  **`asyncio.run(run())`**  is throwing an error only in the ipython Notebook because it seams that it can not be run in the kernel but you should use it in the python script_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Write the data into a jSON file and a pandas DataFrame (Excel file)\n",
    "\n",
    "A pop up will ask you to provide i (for file number to avoid overwriting other files when running the code)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = int(input(\"Enter the file number four the output: \"))\n",
    "# write the data to json file\n",
    "with open(f'product_reviews_{i}.json', 'w') as file:\n",
    "    json.dump(review_data, file, indent=2)\n",
    "# write the data to Excel file\n",
    "df = pd.DataFrame(review_data, columns=['title', 'star_rating', 'verified', 'location', 'date', 'body'])\n",
    "df.to_excel(f\"product_reviews{i}.xlsx\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 ('venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "c241aca8e82980483a0f0acfb405b3e1a2705bee13d54a1a3f8f2cfa39078674"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
